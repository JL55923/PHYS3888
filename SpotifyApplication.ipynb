{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from scipy.signal import find_peaks, peak_prominences\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "def read_arduino(ser,inputBufferSize):\n",
    "#    data = ser.readline(inputBufferSize)\n",
    "    data = ser.read(inputBufferSize)\n",
    "    out =[(int(data[i])) for i in range(0,len(data))]\n",
    "    return out\n",
    "\n",
    "def process_data(data):\n",
    "    data_in = np.array(data)\n",
    "    result = []\n",
    "    i = 1\n",
    "    while i < len(data_in)-1:\n",
    "        if data_in[i] > 127:\n",
    "            # Found beginning of frame\n",
    "            # Extract one sample from 2 bytes\n",
    "            intout = (np.bitwise_and(data_in[i],127))*128\n",
    "            i = i + 1\n",
    "            intout = intout + data_in[i]\n",
    "            result = np.append(result,intout)\n",
    "        i=i+1\n",
    "    return result\n",
    "\n",
    "######### define FFT function first ###########\n",
    "\n",
    "def process_gaussian_fft(t,data_t,sigma_gauss):\n",
    "    nfft = len(data_t) # number of points\n",
    "    dt = t[1]-t[0]  # time interval\n",
    "    maxf = 1/dt     # maximum frequency\n",
    "    df = 1/np.max(t)   # frequency interval\n",
    "    f_fft = np.arange(-maxf/2,maxf/2+df,df)          # define frequency domain\n",
    "\n",
    "    ## DO FFT\n",
    "    data_f = np.fft.fftshift(np.fft.fft(data_t)) # FFT of data\n",
    "\n",
    "    ## GAUSSIAN FILTER\n",
    "#    sigma_gauss = 25  # width of gaussian - defined in the function\n",
    "    gauss_filter = np.exp(-(f_fft)**2/sigma_gauss**2)   # gaussian filter used\n",
    "    data_f_filtered= data_f*gauss_filter    # gaussian filter spectrum in frquency domain\n",
    "    data_t_filtered = np.fft.ifft(np.fft.ifftshift(data_f_filtered))    # bring filtered signal in time domain\n",
    "    return data_t_filtered\n",
    "\n",
    "# Feature extraction\n",
    "\n",
    "def extract_features(data):\n",
    "\n",
    "    # \"data\" needs to already be normalized\n",
    "    sorted_indices = np.argsort(data)\n",
    "    index_pos = sorted_indices[-1:]\n",
    "\n",
    "    negData = -data\n",
    "    sorted_indices = np.argsort(negData)\n",
    "    index_neg = sorted_indices[-1:]\n",
    "\n",
    "    index1 = min(index_pos[0], index_neg[0])\n",
    "    index2 = max(index_pos[0], index_neg[0])\n",
    "\n",
    "    peaksIndexes = [index1, index2]\n",
    "\n",
    "    first_peak = data[index1]\n",
    "    second_peak = data[index2]\n",
    "\n",
    "    # Need a more reliable way to find the first peak. We can't always assume it's the first of second highest peak.\n",
    "\n",
    "    # Feature 1: Is the first peak positive?\n",
    "    is_first_peak_positive = first_peak > 0\n",
    "\n",
    "    # absData = np.abs(data)\n",
    "    \n",
    "    # # Calculate prominences for feature extraction\n",
    "    # prominences = peak_prominences(absData, peaksIndexes)[0]\n",
    "\n",
    "    # # Feature 2: Sharpness of the first peak\n",
    "    # sharpness_first_peak = prominences[0]\n",
    "    \n",
    "    # # Feature 3: Sharpness of the second peak\n",
    "    # sharpness_second_peak = prominences[1]\n",
    "    \n",
    "    # Feature 4: Distance between the two peaks\n",
    "    # distance_between_peaks = np.abs(first_peak - second_peak)\n",
    "    \n",
    "    return np.array([is_first_peak_positive])\n",
    "\n",
    "\n",
    "def read_features_from_folder(folder_path, number_of_features):\n",
    "    feature_matrix = np.empty((0, number_of_features))  # Initialize an empty matrix to store features\n",
    "\n",
    "    # List all files in the directory\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Read the waveform data from each text file\n",
    "            data = np.loadtxt(file_path)\n",
    "\n",
    "\n",
    "            # Extract features from the data\n",
    "            # features = extract_features(data)\n",
    "            features = calculate_means(data, number_of_features)\n",
    "            # Append features as a new row to the feature matrix\n",
    "            feature_matrix = np.vstack((feature_matrix, features))\n",
    "\n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "def calculate_means(array, num_subsets):\n",
    "    # Calculate the size of each subset\n",
    "    n = len(array)\n",
    "    subset_size = n // num_subsets\n",
    "\n",
    "    # Initialize an empty list to store the means\n",
    "    means = []\n",
    "\n",
    "    # Split the array into subsets and calculate the mean of each\n",
    "    for i in range(num_subsets):\n",
    "        start_index = i * subset_size\n",
    "        # Handle the last subset potentially having more elements\n",
    "        if i == num_subsets - 1:\n",
    "            subset = array[start_index:]  # Take everything remaining\n",
    "        else:\n",
    "            subset = array[start_index:start_index + subset_size]\n",
    "        means.append(np.mean(subset))\n",
    "    \n",
    "    return np.array(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynput.keyboard import Controller, Key\n",
    "import time\n",
    "\n",
    "keyboard = Controller()\n",
    "\n",
    "# Function to simulate key press\n",
    "def simulate_key_press(key):\n",
    "    keyboard.press(key)\n",
    "    time.sleep(0.2)  # This is to ensure that the keypress is registered \n",
    "    keyboard.release(key)\n",
    "\n",
    "def press_two_keys(key1, key2):\n",
    "    \"\"\"\n",
    "    Presses two keys simultaneously.\n",
    "    \n",
    "    Args:\n",
    "    key1: The first key to press. Can be a character like 'a' or a special key like Key.ctrl.\n",
    "    key2: The second key to press. Can be a character like 'b' or a special key like Key.alt.\n",
    "    \"\"\"\n",
    "    keyboard = Controller()\n",
    "    \n",
    "    # Press both keys\n",
    "    keyboard.press(key1)\n",
    "    keyboard.press(key2)\n",
    "\n",
    "    # Release both keys\n",
    "    keyboard.release(key1)\n",
    "    keyboard.release(key2)\n",
    "\n",
    "# Read the file and press keys based on specific words\n",
    "def read_and_press_keys(label):\n",
    "    if label == \"pause\": \n",
    "        simulate_key_press(Key.space)  # Simulate space bar press\n",
    "        print(\"Pause\")\n",
    "    elif label == \"right\":\n",
    "        press_two_keys(Key.cmd, Key.right)  # Simulate F9 key press\n",
    "        print(\"Skip song\")\n",
    "    elif label == \"left\":\n",
    "        press_two_keys(Key.cmd, Key.left)  # Simulate F8 key press \n",
    "        print(\"Previous song\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "baudrate = 230400\n",
    "# cport = 'COM9'  # set the correct port before you run it\n",
    "# cport = '/dev/tty.usbmodem141101'  # set the correct port before run it\n",
    "cport = \"/dev/cu.usbserial-DJ00E2W2\"\n",
    "ser = serial.Serial(port=cport, baudrate=baudrate)    # this initializes the animated plot\n",
    "# %matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest training\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# reading in all the eye movement files\n",
    "# storing them as a dataframe with two columns: label and data\n",
    "# label is the eye movement type (ex: up)\n",
    "# data is a list of the numbers that were within the file \n",
    "\n",
    "# whoever is running the code will need to change this \n",
    "\n",
    "main_folder = '/Users/jameslocke/Documents/SpikerStream-master-2/Model training'\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "# iterating over each folder in the main folder\n",
    "\n",
    "for folder_name in os.listdir(main_folder):\n",
    "    folder_path = os.path.join(main_folder, folder_name)\n",
    "    \n",
    "    # if the item is a folder\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        \n",
    "        # iterate over each file in that folder \n",
    "        \n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # if the item is a file \n",
    "            \n",
    "            if os.path.isfile(file_path):\n",
    "                \n",
    "                # store the file content as a list \n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        numbers = file.read().splitlines()\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "                \n",
    "                # creating a dictionary \n",
    "                # with the folder name as the label and file contents as the data \n",
    "                \n",
    "                data.append({'Label': folder_name, 'Data': numbers})\n",
    "\n",
    "# making into a dataframe \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# checking the output \n",
    "\n",
    "print(df.head())\n",
    "print()\n",
    "print(df['Label'])\n",
    "\n",
    "def train_random_forest(dataframe):\n",
    "\n",
    "    X = pd.DataFrame(dataframe['Data'].values.tolist())\n",
    "    print(X)\n",
    "    y = dataframe['Label']\n",
    "    \n",
    "    # handling missing \n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "    \n",
    "    # splitting the data into training and testing sets \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    return clf\n",
    "\n",
    "trained_classifier = train_random_forest(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest classifier\n",
    "\n",
    "def classify_eye_movement(file_path, classifier):\n",
    "\n",
    "    # reading in the file \n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            numbers = file.read().splitlines()\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Unable to read the file due to encoding issues.\")\n",
    "        return None\n",
    "    \n",
    "    # preprocessing the data to match the format expected by the classifier\n",
    "    \n",
    "    # converting the numbers to float values \n",
    "    \n",
    "    try:\n",
    "        numbers = [float(num) for num in numbers]\n",
    "    except ValueError:\n",
    "        print(\"Error converting string data to float values.\")\n",
    "        return None\n",
    "    \n",
    "    # want the file to have the same number of features that the classifier was trained on\n",
    "    # if the number of features is less than expected, pad with zeros \n",
    "    \n",
    "    if len(numbers) <= 29997:\n",
    "        numbers += [0] * (29997 - len(numbers))\n",
    "    \n",
    "    # also address if the features is more than expected\n",
    "    \n",
    "    elif len(numbers) > 29997:\n",
    "        numbers = numbers[:29997]\n",
    "    \n",
    "    # converting to a numpy array with a single row \n",
    "    \n",
    "    X = np.array([numbers])\n",
    "    \n",
    "    predicted_label = classifier.predict(X)[0]\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "# practicing\n",
    "\n",
    "# print(classify_eye_movement('/Users/sfoulsham/Desktop/data3888/testing_examples/r1.txt', \n",
    "#                             trained_classifier))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def build_model(input_dim):\n",
    "    # Create a more complex neural network model\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        # Dropout(0.5),  # Adding dropout regularization\n",
    "        # Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        # Dropout(0.5),  # Adding dropout regularization\n",
    "        Dense(16, activation='relu'),\n",
    "        # Dense(8, activation='relu'),\n",
    "        Dense(3, activation='sigmoid')  # Adjust the number of output units to match the number of classes\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                #   loss = 'binary_crossentropy',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Example of creating a model\n",
    "input_dim = 100  # Adjust this based on the actual number of features you plan to use\n",
    "model = build_model(input_dim)\n",
    "\n",
    "# Specify the path to your folder containing class 1 examples\n",
    "folder_path1 = '/Users/jameslocke/Documents/SpikerStream-master-2/Model training/LookLeft'\n",
    "folder_path2 = '/Users/jameslocke/Documents/SpikerStream-master-2/Model training/LookRight'\n",
    "folder_path3 = '/Users/jameslocke/Documents/SpikerStream-master-2/Model training/Blink'\n",
    "\n",
    "\n",
    "# Read features into a matrix\n",
    "# num_labels_class1 = 31\n",
    "# num_labels_class2 = 26\n",
    "\n",
    "X_class1 = read_features_from_folder(folder_path1, input_dim)\n",
    "X_class2 = read_features_from_folder(folder_path2, input_dim)\n",
    "X_class3 = read_features_from_folder(folder_path3, input_dim)\n",
    "\n",
    "\n",
    "# Create label arrays for class 1 and class 2. 0 for look left, 1 for look right\n",
    "y_class1 = np.zeros((round(np.size(X_class1)/input_dim), 1))\n",
    "y_class2 = np.ones((round(np.size(X_class2)/input_dim), 1))\n",
    "y_class3 = 2*np.ones((round(np.size(X_class3)/input_dim), 1))\n",
    "\n",
    "\n",
    "# Concatenate  arrays for class 1 and class 2\n",
    "X = np.concatenate((X_class1, X_class2, X_class3), axis=0)\n",
    "Y = np.concatenate((y_class1, y_class2, y_class3), axis=0)\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the neural network\n",
    "\n",
    "folder_path = '/Users/jameslocke/Documents/SpikerStream-master-2/Model Practice/dartRight'\n",
    "features = read_features_from_folder(folder_path, 100)\n",
    "\n",
    "predictions = model.predict(features)\n",
    "binary_predictions = (predictions > 0.5).astype(int)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(predicted_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Take 10-20 seconds of static data and send to the \"noise\" folder:\n",
    "\n",
    "path = '/Users/jameslocke/Documents/SpikerStream-master-2/Week 10 data/noise.txt'\n",
    "\n",
    "# To find the mean of the noise, we use:\n",
    "\n",
    "data = np.loadtxt(path) # numpy array of calibration data\n",
    "\n",
    "# Calculate reduced data where deviations from the mean are less than one standard deviation\n",
    "mean_data = np.mean(data)\n",
    "std_data = np.std(data)\n",
    "reduced = data[np.abs(data - mean_data) < 1 * std_data]\n",
    "\n",
    "# Calculate the mean of the reduced data\n",
    "redAve = np.mean(reduced)\n",
    "\n",
    "# The mean of the noise - to be used when centering event waveforms around 0\n",
    "noiseMean = redAve\n",
    "\n",
    "print(noiseMean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take continuous data stream \n",
    "inputBufferSize = 2000 # keep betweein 2000-20000\n",
    "ser.timeout = inputBufferSize/20000.0  # set read timeout, 20000 is one second\n",
    "# ser.set_buffer_size(rx_size = inputBufferSize)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "total_time = 15.0; # time in seconds [[1 s = 20000 buffer size]]\n",
    "max_time = 15; # time plotted in window [s]\n",
    "N_loops = 20000.0/inputBufferSize*total_time\n",
    "\n",
    "T_acquire = inputBufferSize/20000.0    # length of time that data is acquired for \n",
    "N_max_loops = max_time/T_acquire    # total number of loops to cover desire time window\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.ion()\n",
    "# fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "# event detection variables\n",
    "\n",
    "event = 0  # 1 if an event is happening at this time step.\n",
    "events = []\n",
    "event_count = 0\n",
    "\n",
    "event_ended = 0\n",
    "\n",
    "\n",
    "Sd = []\n",
    "Ave = []\n",
    "\n",
    "upThreshold = 10\n",
    "downThreshold = 2\n",
    "gone_up = False\n",
    "just_under = False\n",
    "\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "for k in range(0,int(N_loops)):\n",
    "    data = read_arduino(ser,inputBufferSize)\n",
    "    data_temp = process_data(data)\n",
    "    \n",
    "    # DO THE GAUSSIAN FILTERING FIRST - OTHERWISE YOUR CODE SLOWS DOWN\n",
    "    # define temporary time array OF THE WINDOW\n",
    "    T = inputBufferSize/20000.0*np.linspace(0,1,(data_temp).size)\n",
    "    sigma_gauss = 25\n",
    "    data_temp_filtered = process_gaussian_fft(T,data_temp,sigma_gauss)\n",
    "\n",
    "    if k <= N_max_loops:\n",
    "        if k==0:\n",
    "            data_plot = data_temp\n",
    "            data_plot_filtered = data_temp_filtered\n",
    "        else:\n",
    "            data_plot = np.append(data_temp,data_plot)\n",
    "            data_plot_filtered = np.append(data_temp_filtered,data_plot_filtered)\n",
    "\n",
    "        t = (min(k+1,N_max_loops))*inputBufferSize/20000.0*np.linspace(0,1,(data_plot).size)\n",
    "    else:\n",
    "        data_plot = np.roll(data_plot,len(data_temp))\n",
    "        data_plot[0:len(data_temp)] = data_temp\n",
    "        \n",
    "        data_plot_filtered = np.roll(data_plot_filtered,len(data_temp_filtered))\n",
    "        data_plot_filtered[0:len(data_temp_filtered)] = data_temp_filtered\n",
    "        \n",
    "    t = (min(k+1,N_max_loops))*inputBufferSize/20000.0*np.linspace(0,1,(data_plot).size)\n",
    "    t_filtered = (min(k+1,N_max_loops))*inputBufferSize/20000.0*np.linspace(0,1,(data_plot_filtered).size)\n",
    "        \n",
    "    t_filtered = (min(k+1,N_max_loops))*inputBufferSize/20000.0*np.linspace(0,1,(data_plot_filtered).size)\n",
    "\n",
    "    # same thing here\n",
    "    \n",
    "    ax1.clear()\n",
    "    ax1.set_xlim(0, max_time)\n",
    "    plt.xlabel('time [s]')\n",
    "    ax1.plot(t_filtered,data_plot_filtered)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "\n",
    "    # event detection\n",
    "\n",
    "    data = np.abs(data_temp_filtered)\n",
    "\n",
    "    sd = np.std(data)\n",
    "    ave = np.mean(data)\n",
    "    Sd.append(sd)\n",
    "    Ave.append(ave)\n",
    "\n",
    "    if event == 0:\n",
    "        if sd > upThreshold:  # new event\n",
    "            event_count += 1\n",
    "            event = 1\n",
    "            new_event = data\n",
    "            events.append(new_event)\n",
    "            print(\"new event\")\n",
    "    else:\n",
    "        current_event = events[event_count - 1]\n",
    "        current_event = np.concatenate((current_event, data))\n",
    "        events[event_count - 1] = current_event\n",
    "\n",
    "        # event may have ended if it returns close to mean\n",
    "        # wait until SD turns around until we declare the event to be ended\n",
    "        if sd <= downThreshold:\n",
    "            if not just_under:\n",
    "                just_under = True\n",
    "                under_count = 1\n",
    "            else:\n",
    "                under_count += 1\n",
    "                if under_count >= 1:\n",
    "                    event = 0\n",
    "                    event_ended = 1\n",
    "                    print('End')\n",
    "\n",
    "                    current_event = events[event_count - 1]\n",
    "                    normalised_event = current_event - noiseMean\n",
    "\n",
    "                    features = extract_features(normalised_event)\n",
    "\n",
    "                    prediction = model.predict(features)\n",
    "                    binary_prediction = (prediction > 0.5).astype(int)\n",
    "\n",
    "                    print(binary_prediction)\n",
    "\n",
    "                    if binary_prediction[0][0] == 0:\n",
    "                        print('Left')\n",
    "                        label = 'left'\n",
    "                    elif binary_prediction[0][0] == 1:\n",
    "                        print('Right')\n",
    "                        label = 'right'\n",
    "                    elif binary_prediction[0][0] == 2:\n",
    "                        print('blink')\n",
    "                        label = 'pause'\n",
    "                    \n",
    "                    read_and_press_keys(label)\n",
    "\n",
    "                    # Random forest\n",
    "                    # eventFilePath = '/Users/jameslocke/Documents/SpikerStream-master-2/Week 10 data/current_event.txt' \n",
    "                    # np.savetxt(eventFilePath, normalised_event)\n",
    "                    # print(classify_eye_movement(eventFilePath, trained_classifier))\n",
    "\n",
    "                    \n",
    "\n",
    "            if not gone_up:\n",
    "                gone_up = (np.sign(sd - prev_sd) + 1) / 2  # check for turning around\n",
    "            else:\n",
    "                # event ends if Sd previously went up and now goes down\n",
    "                event = np.sign(sd - prev_sd) == 1\n",
    "\n",
    "                event_ended = 1\n",
    "                print('End')\n",
    "\n",
    "\n",
    "                current_event = events[event_count - 1]\n",
    "                normalised_event = current_event - noiseMean\n",
    "                # normalised_event = current_event\n",
    "\n",
    "                # Neural network\n",
    "\n",
    "                features = extract_features(normalised_event)\n",
    "\n",
    "                prediction = model.predict(features)\n",
    "                binary_prediction = (prediction > 0.5).astype(int)\n",
    "\n",
    "                print(binary_prediction)\n",
    "\n",
    "                if binary_prediction[0][0] == 0:\n",
    "                    print('Left')\n",
    "                    label = 'left'\n",
    "                elif binary_prediction[0][0] == 1:\n",
    "                    print('Right')\n",
    "                    label = 'right'\n",
    "                elif binary_prediction[0][0] == 2:\n",
    "                    print('blink')\n",
    "                    label = 'pause'\n",
    "                \n",
    "                read_and_press_keys(label)\n",
    "\n",
    "                # Random forest\n",
    "\n",
    "                # eventFilePath = '/Users/jameslocke/Documents/SpikerStream-master-2/Week 10 data/current_event.txt' \n",
    "                # np.savetxt(eventFilePath, normalised_event)\n",
    "                # print(classify_eye_movement(eventFilePath, trained_classifier))\n",
    "            \n",
    "\n",
    "\n",
    "                # classify event. Algorithm below:\n",
    "\n",
    "                # check if the first peak is positive or negative. If it's negative, we know it's a look left, or non event.\n",
    "\n",
    "                # If it's positive, \n",
    "\n",
    "            if not event:\n",
    "                gone_up = False\n",
    "                just_under = False\n",
    "        else:\n",
    "            gone_up = False\n",
    "            just_under = False\n",
    "\n",
    "    prev_ave = ave\n",
    "    prev_sd = sd\n",
    "\n",
    "    if event_ended:\n",
    "        current_event = events[event_count - 1]\n",
    "        event_ended = 0\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # if an event ended at this timestep, it would now be fed to the classifier. \n",
    "    # Before doing this, we would also normalise the data, and check the event's amplitude.\n",
    "    # If the amplitude is too small, we could say that it's no longer an event.\n",
    "    # Idea: if event's maximum value isn't more that 2 SD's away from the event's mean, then it's no longer an event.\n",
    "\n",
    "# Need to save t_filtered as well. Also the FFT spectrum for an event.\n",
    "\n",
    "\n",
    "absArray = np.abs(data_plot_filtered)\n",
    "\n",
    "np.savetxt('/Users/jameslocke/Documents/SpikerStream-master-2/Week 7 data/LookLeft/raw_data.txt', data_plot)\n",
    "np.savetxt('/Users/jameslocke/Documents/SpikerStream-master-2/Week 7 data/LookLeft/filtered_data.txt', data_plot_filtered)\n",
    "np.savetxt('/Users/jameslocke/Documents/SpikerStream-master-2/Week 7 data/LookLeft/abs_filtered_data.txt', absArray)\n",
    "\n",
    "i = 0\n",
    "while i < event_count:\n",
    "    currentEvent = events[i]\n",
    "    path = '/Users/jameslocke/Documents/SpikerStream-master-2/Week 10 data/event_number' + str(i) + '.txt'\n",
    "    np.savetxt(path, currentEvent)\n",
    "    i += 1\n",
    "\n",
    "path = '/Users/jameslocke/Documents/SpikerStream-master-2/Week 10 data/noise.txt'\n",
    "np.savetxt(path, absArray)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jameslocke/Documents/SpikerStream-master-2/Week 10 data/noise.txt'\n",
    "np.savetxt(path, absArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jameslocke/Documents/SpikerStream-master-2/Week 10 data/SD.txt'\n",
    "np.savetxt(path, Sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_fft(t,data_t):\n",
    "    nfft = len(data_t) # number of points\n",
    "    dt = t[1]-t[0]  # time interval\n",
    "    maxf = 1/dt     # maximum frequency\n",
    "    df = 1/np.max(t)   # frequency interval\n",
    "    f_fft = np.arange(-maxf/2,maxf/2+df,df)          # define frequency domain\n",
    "\n",
    "    ## DO FFT\n",
    "    data_f = np.fft.fftshift(np.fft.fft(data_t)) # FFT of data\n",
    "\n",
    "    return f_fft, data_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close serial port if necessary\n",
    "if ser.read():\n",
    "    ser.flushInput()\n",
    "    ser.flushOutput()\n",
    "    ser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jameslocke/Documents/SpikerStream-master-2/Week 9 data/SD.txt'\n",
    "np.savetxt(path, Sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative event detection. Doesn't use the turning point or require SD to hang below threshold for a period of time. \n",
    "# More suitable for larger buffer size.\n",
    "\n",
    "\n",
    "# take continuous data stream \n",
    "inputBufferSize = 4000 # keep betweein 2000-20000\n",
    "ser.timeout = inputBufferSize/20000.0  # set read timeout, 20000 is one second\n",
    "# ser.set_buffer_size(rx_size = inputBufferSize)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "total_time = 15.0; # time in seconds [[1 s = 20000 buffer size]]\n",
    "max_time = 15; # time plotted in window [s]\n",
    "N_loops = 20000.0/inputBufferSize*total_time\n",
    "\n",
    "T_acquire = inputBufferSize/20000.0    # length of time that data is acquired for \n",
    "N_max_loops = max_time/T_acquire    # total number of loops to cover desire time window\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.ion()\n",
    "# fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "# event detection variables\n",
    "\n",
    "event = 0  # 1 if an event is happening at this time step.\n",
    "events = []\n",
    "event_count = 0\n",
    "\n",
    "event_ended = 0\n",
    "\n",
    "\n",
    "Sd = []\n",
    "Ave = []\n",
    "\n",
    "upThreshold = 10\n",
    "downThreshold = 5\n",
    "gone_up = False\n",
    "just_under = False\n",
    "\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "for k in range(0,int(N_loops)):\n",
    "    data = read_arduino(ser,inputBufferSize)\n",
    "    data_temp = process_data(data)\n",
    "    \n",
    "    # DO THE GAUSSIAN FILTERING FIRST - OTHERWISE YOUR CODE SLOWS DOWN\n",
    "    # define temporary time array OF THE WINDOW\n",
    "    T = inputBufferSize/20000.0*np.linspace(0,1,(data_temp).size)\n",
    "    sigma_gauss = 25\n",
    "    data_temp_filtered = process_gaussian_fft(T,data_temp,sigma_gauss)\n",
    "\n",
    "    if k <= N_max_loops:\n",
    "        if k==0:\n",
    "            data_plot = data_temp\n",
    "            data_plot_filtered = data_temp_filtered\n",
    "        else:\n",
    "            data_plot = np.append(data_temp,data_plot)\n",
    "            data_plot_filtered = np.append(data_temp_filtered,data_plot_filtered)\n",
    "\n",
    "        t = (min(k+1,N_max_loops))*inputBufferSize/20000.0*np.linspace(0,1,(data_plot).size)\n",
    "    else:\n",
    "        data_plot = np.roll(data_plot,len(data_temp))\n",
    "        data_plot[0:len(data_temp)] = data_temp\n",
    "        \n",
    "        data_plot_filtered = np.roll(data_plot_filtered,len(data_temp_filtered))\n",
    "        data_plot_filtered[0:len(data_temp_filtered)] = data_temp_filtered\n",
    "        \n",
    "    t = (min(k+1,N_max_loops))*inputBufferSize/20000.0*np.linspace(0,1,(data_plot).size)\n",
    "    t_filtered = (min(k+1,N_max_loops))*inputBufferSize/20000.0*np.linspace(0,1,(data_plot_filtered).size)\n",
    "        \n",
    "    t_filtered = (min(k+1,N_max_loops))*inputBufferSize/20000.0*np.linspace(0,1,(data_plot_filtered).size)\n",
    "\n",
    "    # same thing here\n",
    "    \n",
    "    ax1.clear()\n",
    "    ax1.set_xlim(0, max_time)\n",
    "    plt.xlabel('time [s]')\n",
    "    ax1.plot(t_filtered,data_plot_filtered)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "\n",
    "    # event detection\n",
    "\n",
    "    data = np.abs(data_temp_filtered)\n",
    "\n",
    "    sd = np.std(data)\n",
    "    ave = np.mean(data)\n",
    "    Sd.append(sd)\n",
    "    Ave.append(ave)\n",
    "\n",
    "    if event == 0:\n",
    "        if sd > upThreshold:  # new event\n",
    "            event_count += 1\n",
    "            event = 1\n",
    "            new_event = data\n",
    "            events.append(new_event)\n",
    "            print(\"new event\")\n",
    "    else:\n",
    "        if sd <= downThreshold:\n",
    "            event = 0\n",
    "            print('End')\n",
    "\n",
    "            current_event = events[event_count - 1]\n",
    "            normalised_event = current_event - noiseMean\n",
    "\n",
    "            features = extract_features(normalised_event)\n",
    "\n",
    "            prediction = model.predict(features)\n",
    "            binary_prediction = (prediction > 0.5).astype(int)\n",
    "\n",
    "            if binary_prediction[0][0] == 0:\n",
    "                print('Left')\n",
    "                label = 'left'\n",
    "            elif binary_prediction[0][0] == 1:\n",
    "                print('Right')\n",
    "                label = 'right'\n",
    "            elif binary_prediction[0][0] == 2:\n",
    "                print('blink')\n",
    "                label = 'pause'\n",
    "            \n",
    "            read_and_press_keys(label)\n",
    "\n",
    "        else:\n",
    "            current_event = events[event_count - 1]\n",
    "            current_event = np.concatenate((current_event, data))\n",
    "            events[event_count - 1] = current_event\n",
    "                \n",
    "    prev_ave = ave\n",
    "    prev_sd = sd\n",
    "\n",
    "\n",
    "    # if an event ended at this timestep, it would now be fed to the classifier. \n",
    "    # Before doing this, we would also normalise the data, and check the event's amplitude.\n",
    "    # If the amplitude is too small, we could say that it's no longer an event.\n",
    "    # Idea: if event's maximum value isn't more that 2 SD's away from the event's mean, then it's no longer an event.\n",
    "\n",
    "# Need to save t_filtered as well. Also the FFT spectrum for an event.\n",
    "\n",
    "\n",
    "absArray = np.abs(data_plot_filtered)\n",
    "\n",
    "np.savetxt('/Users/jameslocke/Documents/SpikerStream-master-2/Week 7 data/LookLeft/raw_data.txt', data_plot)\n",
    "np.savetxt('/Users/jameslocke/Documents/SpikerStream-master-2/Week 7 data/LookLeft/filtered_data.txt', data_plot_filtered)\n",
    "np.savetxt('/Users/jameslocke/Documents/SpikerStream-master-2/Week 7 data/LookLeft/abs_filtered_data.txt', absArray)\n",
    "\n",
    "i = 0\n",
    "while i < event_count:\n",
    "    currentEvent = events[i]\n",
    "    path = '/Users/jameslocke/Documents/SpikerStream-master-2/Week 10 data/event_number' + str(i) + '.txt'\n",
    "    np.savetxt(path, currentEvent)\n",
    "    i += 1\n",
    "\n",
    "path = '/Users/jameslocke/Documents/SpikerStream-master-2/Week 10 data/noise.txt'\n",
    "np.savetxt(path, absArray)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
