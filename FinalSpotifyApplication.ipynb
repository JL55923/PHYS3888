{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from scipy.signal import find_peaks, peak_prominences\n",
    "import scipy.signal\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "def read_arduino(ser,inputBufferSize):\n",
    "#    data = ser.readline(inputBufferSize)\n",
    "    data = ser.read(inputBufferSize)\n",
    "    out =[(int(data[i])) for i in range(0,len(data))]\n",
    "    return out\n",
    "\n",
    "def process_data(data):\n",
    "    data_in = np.array(data)\n",
    "    result = []\n",
    "    i = 1\n",
    "    while i < len(data_in)-1:\n",
    "        if data_in[i] > 127:\n",
    "            # Found beginning of frame\n",
    "            # Extract one sample from 2 bytes\n",
    "            intout = (np.bitwise_and(data_in[i],127))*128\n",
    "            i = i + 1\n",
    "            intout = intout + data_in[i]\n",
    "            result = np.append(result,intout)\n",
    "        i=i+1\n",
    "    return result\n",
    "\n",
    "######### define FFT function first ###########\n",
    "\n",
    "def process_gaussian_fft(t,data_t,sigma_gauss):\n",
    "    nfft = len(data_t) # number of points\n",
    "    dt = t[1]-t[0]  # time interval\n",
    "    maxf = 1/dt     # maximum frequency\n",
    "    df = 1/np.max(t)   # frequency interval\n",
    "    f_fft = np.arange(-maxf/2,maxf/2+df,df)          # define frequency domain\n",
    "\n",
    "    ## DO FFT\n",
    "    data_f = np.fft.fftshift(np.fft.fft(data_t)) # FFT of data\n",
    "\n",
    "    ## GAUSSIAN FILTER\n",
    "#    sigma_gauss = 25  # width of gaussian - defined in the function\n",
    "    gauss_filter = np.exp(-(f_fft)**2/sigma_gauss**2)   # gaussian filter used\n",
    "    data_f_filtered= data_f*gauss_filter    # gaussian filter spectrum in frquency domain\n",
    "    data_t_filtered = np.fft.ifft(np.fft.ifftshift(data_f_filtered))    # bring filtered signal in time domain\n",
    "    return data_t_filtered\n",
    "\n",
    "# Feature extraction for Random forest\n",
    "def extract_features(data):\n",
    "\n",
    "    # \"data\" needs to already be normalized\n",
    "    peaksIndexes = [np.argmax(data), np.argmin(data)]\n",
    "\n",
    "    # indexes of peak and trough (in the order they occur)\n",
    "    peaksIndexes = [min(peaksIndexes), max(peaksIndexes)]\n",
    "\n",
    "    # Normalise data\n",
    "    absData = np.abs(data) / max(data)\n",
    "\n",
    "    area = calculate_signed_area(absData)\n",
    "\n",
    "    max_min_ratio = np.abs(np.max(data) / np.min(data))\n",
    "\n",
    "    widths = scipy.signal.peak_widths(absData, peaksIndexes)\n",
    "    width1 = widths[0][0]\n",
    "    width2 = widths[0][1]\n",
    "\n",
    "    return np.array([area, max_min_ratio, width1, width2])\n",
    "\n",
    "\n",
    "def calculate_signed_area(array):\n",
    "    signed_area = np.trapz(array, dx=1)\n",
    "    return signed_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynput.keyboard import Controller, Key\n",
    "import time\n",
    "\n",
    "keyboard = Controller()\n",
    "\n",
    "# Function to simulate key press\n",
    "def simulate_key_press(key):\n",
    "    keyboard.press(key)\n",
    "    time.sleep(0.2)  # This is to ensure that the keypress is registered \n",
    "    keyboard.release(key)\n",
    "\n",
    "def press_two_keys(key1, key2):\n",
    "    \"\"\"\n",
    "    Presses two keys simultaneously.\n",
    "    \n",
    "    Args:\n",
    "    key1: The first key to press. Can be a character like 'a' or a special key like Key.ctrl.\n",
    "    key2: The second key to press. Can be a character like 'b' or a special key like Key.alt.\n",
    "    \"\"\"\n",
    "    keyboard = Controller()\n",
    "    \n",
    "    # Press both keys\n",
    "    keyboard.press(key1)\n",
    "    keyboard.press(key2)\n",
    "\n",
    "    # Release both keys\n",
    "    keyboard.release(key1)\n",
    "    keyboard.release(key2)\n",
    "\n",
    "# Read the file and press keys based on specific words\n",
    "def read_and_press_keys(label):\n",
    "    if label == \"pause\": \n",
    "        simulate_key_press(Key.space)  # Simulate space bar press\n",
    "        print(\"Pause\")\n",
    "    elif label == \"right\":\n",
    "        press_two_keys(Key.cmd, Key.right)  # Simulate F9 key press\n",
    "        print(\"Skip song\")\n",
    "    elif label == \"left\":\n",
    "        press_two_keys(Key.cmd, Key.left)  # Simulate F8 key press \n",
    "        print(\"Previous song\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to find ports\n",
    "from serial.tools import list_ports\n",
    "\n",
    "ports = list_ports.comports()\n",
    "for port in ports:\n",
    "    print(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "baudrate = 230400\n",
    "# cport = 'COM9'  # set the correct port before you run it\n",
    "# cport = '/dev/tty.usbmodem141101'  # set the correct port before run it\n",
    "cport = \"/dev/cu.usbserial-DM02JHCY\"\n",
    "ser = serial.Serial(port=cport, baudrate=baudrate)    # this initializes the animated plot\n",
    "# %matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest training\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# reading in all the eye movement files\n",
    "# storing them as a dataframe with two columns: label and data\n",
    "# label is the eye movement type (ex: up)\n",
    "# data is a list of the numbers that were within the file \n",
    "\n",
    "# whoever is running the code will need to change this \n",
    "\n",
    "main_folder = '/Users/jameslocke/Documents/SpikerStream-master-2/Model training/finalTrainingExamples'\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "# iterating over each folder in the main folder\n",
    "\n",
    "for folder_name in os.listdir(main_folder):\n",
    "    folder_path = os.path.join(main_folder, folder_name)\n",
    "    \n",
    "    # if the item is a folder\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        \n",
    "        # iterate over each file in that folder \n",
    "        \n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # if the item is a file \n",
    "            \n",
    "            if os.path.isfile(file_path):\n",
    "                \n",
    "                # store the file content as a list \n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        numbers = file.read().splitlines()\n",
    "                        nump_array = np.array(list(np.float_(numbers)))\n",
    "                        features=  extract_features(nump_array)\n",
    "                        numbers = list(features)\n",
    "                        numbers = list(map(str, numbers))\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "                \n",
    "                # creating a dictionary \n",
    "                # with the folder name as the label and file contents as the data \n",
    "                \n",
    "                # data.append({'Label': folder_name, 'Data': numbers})\n",
    "                data.append({'Label': folder_name, 'Data': features})\n",
    "\n",
    "# making into a dataframe \n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# checking the output \n",
    "\n",
    "print(df.head())\n",
    "print()\n",
    "print(df['Label'])\n",
    "\n",
    "def train_random_forest(dataframe):\n",
    "\n",
    "    X = pd.DataFrame(dataframe['Data'].values.tolist())\n",
    "    y = dataframe['Label']\n",
    "    print(X)\n",
    "    \n",
    "    # handling missing \n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "    \n",
    "    # splitting the data into training and testing sets \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    return clf, scaler\n",
    "\n",
    "outputs = train_random_forest(df)\n",
    "trained_classifier = outputs[0]\n",
    "scaler = outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest classifier for a file\n",
    "def classify_eye_movement(file_path, classifier, scaler):\n",
    "\n",
    "    # reading in the file \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            numbers = file.read().splitlines()\n",
    "            nump_array = np.array(list(np.float_(numbers)))\n",
    "            features=  extract_features(nump_array)\n",
    "            # numbers = list(features)\n",
    "            # numbers = list(map(str, numbers))\n",
    "\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"Unable to read the file due to encoding issues.\")\n",
    "        return None\n",
    "        \n",
    "    # converting the numbers to float values \n",
    "    \n",
    "    try:\n",
    "        numbers = [float(num) for num in numbers]\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"Error converting string data to float values.\")\n",
    "        return None\n",
    "    \n",
    "    # converting to a numpy array with a single row \n",
    "        \n",
    "    X = np.array([features])\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    # Predict the label\n",
    "    predicted_label = classifier.predict(X_scaled)[0]\n",
    "\n",
    "    \n",
    "    # predicted_label = classifier.predict(X)[0]\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "# Live classifier\n",
    "def live_classify(normalised_event, classifier, scaler):\n",
    "    features = extract_features(normalised_event)\n",
    "\n",
    "    X = np.array([features])\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    # Predict the label\n",
    "    predicted_label = classifier.predict(X_scaled)[0]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Blink vs double blink\n",
    "def distinguish_signal(signal, significance_threshold = 0.5):\n",
    "    # Find peaks in the signal\n",
    "    peaks, _ = find_peaks(signal, prominence=(None, None))\n",
    "\n",
    "    # If there are at least two peaks\n",
    "    if len(peaks) >= 2:\n",
    "        # Sort peaks by prominence (peak height)\n",
    "        sorted_peaks = sorted(peaks, key=lambda x: signal[x], reverse=True)\n",
    "        first_peak = sorted_peaks[0]\n",
    "        second_peak = sorted_peaks[1]\n",
    "\n",
    "        # Check if the second peak is significant compared to the first peak\n",
    "        significance = signal[second_peak] / signal[first_peak]\n",
    "        \n",
    "        if significance > significance_threshold:\n",
    "            return \"Type 2 Signal\"  # Second peak significant\n",
    "        else:\n",
    "            return \"Type 1 Signal\"  # Second peak not significant\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511.57372136738365\n"
     ]
    }
   ],
   "source": [
    "# Calibration - 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Firstly, take 20s of data to calculate the noise mean. This is now stored in data_plot_filtered\n",
    "\n",
    "# numpy array of calibration data\n",
    "data = np.abs(data_plot_filtered) \n",
    "\n",
    "# Calculate reduced data where deviations from the mean are less than one standard deviation\n",
    "mean_data = np.mean(data)\n",
    "std_data = np.std(data)\n",
    "reduced = data[np.abs(data - mean_data) < 1 * std_data]\n",
    "\n",
    "# Calculate the mean of the reduced data\n",
    "redAve = np.mean(reduced)\n",
    "\n",
    "# The mean of the noise - to be used when centering event waveforms around 0\n",
    "noiseMean = redAve\n",
    "\n",
    "print(noiseMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration - 2\n",
    "\n",
    "# Secondly, take 30s of data with a certain sequence of events\n",
    "\n",
    "# Events are all normalised\n",
    "Vpp = []\n",
    "\n",
    "i = 0\n",
    "while i < event_count:\n",
    "    currentEvent = events[i]\n",
    "    Vpp.append(max(currentEvent) - min(currentEvent))\n",
    "    i += 1\n",
    "\n",
    "aveVpp = sum(Vpp) / len(Vpp)\n",
    "\n",
    "# Find the minimum Vpp to not be considered a false positive\n",
    "if min(Vpp) < aveVpp:\n",
    "    minimum_Vpp = min(Vpp) / 2\n",
    "else:\n",
    "    minimum_Vpp = aveVpp / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take continuous data stream \n",
    "inputBufferSize = 10000 # keep betweein 2000-20000\n",
    "ser.timeout = inputBufferSize/20000.0  # set read timeout, 20000 is one second\n",
    "# ser.set_buffer_size(rx_size = inputBufferSize)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "total_time = 20.0; # time in seconds [[1 s = 20000 buffer size]]\n",
    "max_time = 20; # time plotted in window [s]\n",
    "N_loops = 20000.0/inputBufferSize*total_time\n",
    "\n",
    "T_acquire = inputBufferSize/20000.0    # length of time that data is acquired for \n",
    "N_max_loops = max_time/T_acquire    # total number of loops to cover desire time window\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.ion()\n",
    "# fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "# event detection variables\n",
    "\n",
    "event = 0  # 1 if an event is happening at this time step.\n",
    "events = []\n",
    "event_count = 0\n",
    "\n",
    "event_ended = 0\n",
    "\n",
    "upThreshold = 10\n",
    "downThreshold = 5\n",
    "# downThreshold = 8\n",
    "\n",
    "gone_up = False\n",
    "just_under = False\n",
    "\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "for k in range(0,int(N_loops)):\n",
    "    data = read_arduino(ser,inputBufferSize)\n",
    "    data_temp = process_data(data)\n",
    "    \n",
    "    # DO THE GAUSSIAN FILTERING FIRST - OTHERWISE YOUR CODE SLOWS DOWN\n",
    "    # define temporary time array OF THE WINDOW\n",
    "    T = inputBufferSize/20000.0*np.linspace(0,1,(data_temp).size)\n",
    "    sigma_gauss = 25\n",
    "    data_temp_filtered = process_gaussian_fft(T,data_temp,sigma_gauss)\n",
    "\n",
    "    if k <= N_max_loops:\n",
    "        if k==0:\n",
    "            data_plot = data_temp\n",
    "            data_plot_filtered = data_temp_filtered\n",
    "        else:\n",
    "            data_plot = np.append(data_temp,data_plot)\n",
    "            data_plot_filtered = np.append(data_temp_filtered,data_plot_filtered)\n",
    "\n",
    "        t = (min(k+1,N_max_loops))*inputBufferSize/20000.0*np.linspace(0,1,(data_plot).size)\n",
    "    else:\n",
    "        data_plot = np.roll(data_plot,len(data_temp))\n",
    "        data_plot[0:len(data_temp)] = data_temp\n",
    "        \n",
    "        data_plot_filtered = np.roll(data_plot_filtered,len(data_temp_filtered))\n",
    "        data_plot_filtered[0:len(data_temp_filtered)] = data_temp_filtered\n",
    "        \n",
    "    t = (min(k+1,N_max_loops))*inputBufferSize/20000.0*np.linspace(0,1,(data_plot).size)\n",
    "    t_filtered = (min(k+1,N_max_loops))*inputBufferSize/20000.0*np.linspace(0,1,(data_plot_filtered).size)\n",
    "        \n",
    "    t_filtered = (min(k+1,N_max_loops))*inputBufferSize/20000.0*np.linspace(0,1,(data_plot_filtered).size)\n",
    "\n",
    "    # same thing here\n",
    "    \n",
    "    ax1.clear()\n",
    "    ax1.set_xlim(0, max_time)\n",
    "    plt.xlabel('time [s]')\n",
    "    ax1.plot(t_filtered,data_plot_filtered)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "\n",
    "    # event detection\n",
    "\n",
    "    data = np.abs(data_temp_filtered)\n",
    "    sd = np.std(data)\n",
    "\n",
    "    if event == 0:\n",
    "        if sd > upThreshold:  # new event\n",
    "            event_count += 1\n",
    "            event = 1\n",
    "            new_event = data\n",
    "            events.append(new_event)\n",
    "            print(\"new event\")\n",
    "    else:\n",
    "        if sd <= downThreshold:\n",
    "            event = 0\n",
    "            \n",
    "            current_event = events[event_count - 1]\n",
    "            normalised_event = current_event - noiseMean\n",
    "            events[event_count - 1] = normalised_event\n",
    "\n",
    "            # Consider filtering out events that are too short or not large enough in ampltiude. \n",
    "            # Possible idea: if event's maximum value isn't more that 2 SD's away from the event's mean, then it's no longer an event.\n",
    "\n",
    "            # Use calibration to set a reasonable value for this peak to peak voltage. E.g. get user to look right several times, then left\n",
    "            # several times. Take the average of these peak to peak voltages, and set the threshold to be half. Or, just set the threshold\n",
    "            # to be half of the minimum Vpp that we observe during calibration. Should definitely take a combination of all possible \n",
    "            # movements to be used, i.e. take blinks as well (probably don't need to worry about double blinks)\n",
    "            minimum_Vpp = 50\n",
    "\n",
    "            if len(normalised_event) < 4000 or (max(normalised_event) - min(normalised_event) < minimum_Vpp):\n",
    "                print('non event')\n",
    "            else:\n",
    "                \n",
    "                # Random forest\n",
    "                event_type = live_classify(normalised_event, trained_classifier, scaler)\n",
    "                \n",
    "                print(event_type)\n",
    "\n",
    "                # determine using max/min if event is left or right (and give the correct label)\n",
    "                # Also need a way to distinguish between single and double blink.\n",
    "\n",
    "                if event_type == \"Left\" or event_type == \"Right\":\n",
    "                    peak_index = np.argmax(data)\n",
    "                    trough_index = np.argmax(data)\n",
    "\n",
    "                    if peak_index < trough_index:\n",
    "                        label = \"left\"\n",
    "                    else:\n",
    "                        label = \"right\"\n",
    "                else:\n",
    "                    label = event_type\n",
    "\n",
    "                # read_and_press_keys(label)\n",
    "\n",
    "        else:\n",
    "            current_event = events[event_count - 1]\n",
    "            current_event = np.concatenate((current_event, data))\n",
    "            events[event_count - 1] = current_event\n",
    "                \n",
    "    prev_ave = ave\n",
    "    prev_sd = sd\n",
    "\n",
    "# absArray = np.abs(data_plot_filtered)\n",
    "# combined_array = np.column_stack((t_filtered, absArray))\n",
    "# path = '/Users/jameslocke/Documents/SpikerStream-master-2/Week 12 data/lookRightTest.txt'\n",
    "\n",
    "# # Save the combined array to a CSV file\n",
    "# np.savetxt(path, combined_array, delimiter=',')\n",
    "\n",
    "# i = 0\n",
    "# while i < event_count:\n",
    "#     currentEvent = events[i]\n",
    "#     path = '/Users/jameslocke/Documents/SpikerStream-master-2/Week 12 data/Events/event_number' + str(i) + '.txt'\n",
    "#     np.savetxt(path, currentEvent)\n",
    "#     i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_fft(t,data_t):\n",
    "    nfft = len(data_t) # number of points\n",
    "    dt = t[1]-t[0]  # time interval\n",
    "    maxf = 1/dt     # maximum frequency\n",
    "    df = 1/np.max(t)   # frequency interval\n",
    "    f_fft = np.arange(-maxf/2,maxf/2+df,df)          # define frequency domain\n",
    "\n",
    "    ## DO FFT\n",
    "    data_f = np.fft.fftshift(np.fft.fft(data_t)) # FFT of data\n",
    "\n",
    "    return f_fft, data_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close serial port if necessary\n",
    "if ser.read():\n",
    "    ser.flushInput()\n",
    "    ser.flushOutput()\n",
    "    ser.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
